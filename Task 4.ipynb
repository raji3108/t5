{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOonDGwdwj+VTdrXEjPxJ15"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"way6Isqyl7KB"},"outputs":[],"source":["!pip install transformers"]},{"cell_type":"code","source":["from transformers import pipeline, set_seed\n","\n","# Load GPT-2 text generation pipeline\n","generator = pipeline('text-generation', model='gpt2')\n","set_seed(42)  # for consistent results"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fM3PExUJoHaW","executionInfo":{"status":"ok","timestamp":1747139474540,"user_tz":-330,"elapsed":869,"user":{"displayName":"Gadeela dinesh Reddy","userId":"11553965355951621502"}},"outputId":"1b2d22b1-3283-4fd0-8c7d-797859b0095e"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["Device set to use cpu\n"]}]},{"cell_type":"code","source":["# Take user prompt\n","prompt = input(\"Enter your prompt to generate text:\\n\")\n","\n","# Generate text\n","results = generator(prompt, max_length=150, num_return_sequences=1)\n","\n","# Show result\n","print(\"\\nGenerated Text:\\n\")\n","print(results[0]['generated_text'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_iqnHa4uoHlh","executionInfo":{"status":"ok","timestamp":1747139509502,"user_tz":-330,"elapsed":25473,"user":{"displayName":"Gadeela dinesh Reddy","userId":"11553965355951621502"}},"outputId":"775a3c78-4ed3-4d45-99dd-e07b23d0984d"},"execution_count":6,"outputs":[{"name":"stdout","output_type":"stream","text":["Enter your prompt to generate text:\n","How to get wether details \n"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","Generated Text:\n","\n","How to get wether details .................................... A: Find the list of information about the system on my wiki with the URL, name and the \"name\" of the user so that you can get the whole story. B: Find the name of the user in the list. C: Find that user within the list. D: Find that user in the list once you have the name. E: Find out about the wiki description after you've read it, and if you like, add it to your comment section. - https://docs.google.com/document/d/1Gg9FpTQZLzB2TJqSQk7p4L0v2XlHm_p\n"]}]}]}